{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2 : Pre-Processing of Job Description Text</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Import Packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import regex as re\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# If you haven't already done so, execute:\n",
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Load data from sqlite database</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data']\n",
      "['jobtitle', 'company', 'location', 'salary', 'jobdescription', 'label']\n",
      "Table('data', MetaData(bind=None), Column('jobtitle', VARCHAR(length=100), table=<data>), Column('company', VARCHAR(length=100), table=<data>), Column('location', VARCHAR(length=25), table=<data>), Column('salary', INTEGER(), table=<data>), Column('jobdescription', TEXT(), table=<data>), Column('label', INTEGER(), table=<data>), schema=None)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "\n",
    "# Query SQL db to get analyst job descriptions first\n",
    "\n",
    "# Create connection to db\n",
    "engine = create_engine(\"sqlite:///joblist.sqlite\")\n",
    "print(engine.table_names())\n",
    "\n",
    "# Load in data table\n",
    "metadata = MetaData()\n",
    "data = Table('data', metadata, autoload=True, autoload_with=engine)\n",
    "\n",
    "print(data.columns.keys())\n",
    "print(repr(metadata.tables['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jobdescription', 'label']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "# Build query\n",
    "stmt = select([data.columns.jobdescription, data.columns.label])\n",
    "stmt = stmt.where(data.columns.label == '0') # 0 = analysts\n",
    "\n",
    "# Create connection to engine\n",
    "connection = engine.connect()\n",
    "\n",
    "# Execute query\n",
    "results = connection.execute(stmt).fetchall()\n",
    "print(results[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in db\n",
    "\n",
    "from sqlalchemy import func\n",
    "\n",
    "stmt_count = select([func.count(data.columns.jobdescription)])\n",
    "results_count = connection.execute(stmt_count).scalar()\n",
    "print(results_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      jobdescription  label\n",
      "0  Position Title:Pricing Analyst Position Type: ...      0\n",
      "1  Title: Senior Data Analyst - Telephony Manager...      0\n",
      "2  We are looking for a talented Fuel Cell Data E...      0\n",
      "3  CAREER OPPORTUNITY SENIOR METER DATA ANALYST L...      0\n",
      "4  The Data Engineer reports directly to the Dire...      0\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe from SQLAlchemy ResultSet\n",
    "df_data = pd.DataFrame(results)\n",
    "\n",
    "# Give columns proper heading\n",
    "df_data.columns = results[0].keys()\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 450 entries, 0 to 449\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   jobdescription  450 non-null    object\n",
      " 1   label           450 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 7.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 450 entries, 0 to 449\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   jobdescription  450 non-null    string\n",
      " 1   label           450 non-null    int64 \n",
      "dtypes: int64(1), string(1)\n",
      "memory usage: 7.2 KB\n"
     ]
    }
   ],
   "source": [
    "df_data['jobdescription'] = df_data['jobdescription'].astype('string')\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Analysis of Job Description</h2>\n",
    "\n",
    "<h3>Pre-processing Pipeline</h3>\n",
    "\n",
    "For each job description, we will tokenize, stem, remove stop words, and lowercase each word.\n",
    "\n",
    "The following is an example using 1 job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select an example job description from df_data\n",
    "\n",
    "text = df_data['jobdescription'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n"
     ]
    }
   ],
   "source": [
    "# Tokenize example text\n",
    "# Maybe add len(token) to dataframe and plot\n",
    "\n",
    "text_token = word_tokenize(str(text))\n",
    "print(len(text_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'Preferred', ')', 'Work', 'remotely', ':', 'Temporarily', 'due', 'to', 'COVID-19COVID-19', 'precaution', '(', 's', ')', ':', 'Remote', 'interview', 'processPersonal', 'protective', 'equipment', 'provided', 'or', 'requiredPlastic', 'shield', 'at', 'work', 'stationsSocial', 'distancing', 'guidelines', 'in', 'placeVirtual', 'meetingsSanitizing', ',', 'disinfecting', ',', 'or', 'cleaning', 'procedures', 'in', 'place']\n"
     ]
    }
   ],
   "source": [
    "print(text_token[-40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'prefer', ')', 'work', 'remot', ':', 'temporarili', 'due', 'to', 'covid-19covid-19', 'precaut', '(', 's', ')', ':', 'remot', 'interview', 'processperson', 'protect', 'equip', 'provid', 'or', 'requiredplast', 'shield', 'at', 'work', 'stationssoci', 'distanc', 'guidelin', 'in', 'placevirtu', 'meetingssanit', ',', 'disinfect', ',', 'or', 'clean', 'procedur', 'in', 'place']\n"
     ]
    }
   ],
   "source": [
    "# stem tokens\n",
    "# stemmer takes a list an input\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text_stemmed = [stemmer.stem(w) for w in text_token]\n",
    "print(text_stemmed[-40:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Pre-processing Pipeline</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the job postings currently available will be considered the \"test\" set and will not be split. Any predictions will be made on new postings scraped at a later date.\n",
    "\n",
    "CountVectorizer and TF-IDF are two methods that can be used to vectorize text.\n",
    "\n",
    "The CountVectorizer function returns an encoded vector with an integer count for each word.\n",
    "\n",
    "We will use CountVectorizer to identify skills that are commonly found in job descriptions. These skills would therefore have a higher count across the corpus compared to other (i) lesser used or company-specific skills and (ii) non-important words.\n",
    "\n",
    "\n",
    "The pre-processing pipeline will include the following steps:\n",
    "\n",
    "- Tokenization\n",
    "- Stopword removal\n",
    "- Lower casing\n",
    "- Apply transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TFIDF Vectorizer</h2>\n",
    "\n",
    "TFIDF converts the job description text into a matrix of TF-IDF features. The TF-IDF Vectorizer function from the sklearn.feature_extraction module performs multiple steps including tokenization, stopword removal, and lower casing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1. Initialize Stopword Removal & Lowercasing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words to stopwords list\n",
    "# https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list\n",
    "custom_stopwords = ['bachelor', 'degree', 'work', 'equal', 'opportunity', 'employer', 'objectives', 'ontario', 'canada', 'disability', 'strong', 'including', 'ensure', 'understanding', 'related']\n",
    "\n",
    "# Initialize TFIDF Vectorizer\n",
    "tvec = TfidfVectorizer(analyzer = 'word',\n",
    "                       stop_words = ENGLISH_STOP_WORDS.union(custom_stopwords), \n",
    "                       lowercase= True, \n",
    "                       min_df=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Apply TF-IDF</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns matrix of tf-idf features\n",
    "tvec_token = tvec.fit_transform(df_data['jobdescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 3908)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  tfidf\n",
      "data           1.008909\n",
      "experience     1.061734\n",
      "business       1.105114\n",
      "skills         1.115015\n",
      "team           1.137658\n",
      "management     1.242170\n",
      "analysis       1.256395\n",
      "years          1.259265\n",
      "analyst        1.327642\n",
      "support        1.330724\n",
      "process        1.333815\n",
      "knowledge      1.355725\n",
      "ability        1.374895\n",
      "working        1.391156\n",
      "requirements   1.431295\n",
      "communication  1.434714\n",
      "tools          1.458978\n",
      "development    1.469560\n",
      "solutions      1.491066\n",
      "job            1.509349\n",
      "role           1.516756\n",
      "analytical     1.516756\n",
      "provide        1.524219\n",
      "environment    1.524219\n",
      "information    1.539313\n"
     ]
    }
   ],
   "source": [
    "# Observe TFIDF Weights\n",
    "\n",
    "weights = dict(zip(tvec.get_feature_names(), tvec.idf_))\n",
    "tfidf = pd.DataFrame.from_dict(weights, orient='index')\n",
    "tfidf.columns = ['tfidf']\n",
    "\n",
    "# Lowest TFIDF Scores\n",
    "low_tfidf = tfidf.sort_values(by=['tfidf'], ascending=True).head(25)\n",
    "print(low_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tfidf\n",
      "œuvre       5.502029\n",
      "colleague   5.502029\n",
      "golden      5.502029\n",
      "sites       5.502029\n",
      "catalog     5.502029\n",
      "glassdoor   5.502029\n",
      "gl          5.502029\n",
      "unwavering  5.502029\n",
      "cats        5.502029\n",
      "cc          5.502029\n"
     ]
    }
   ],
   "source": [
    "# Highest TFIDF Scores\n",
    "high_tfidf = tfidf.sort_values(by=['tfidf'], ascending=False).head(10)\n",
    "print(high_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bi-grams and Tri-grams</h3>\n",
    "\n",
    "The TF-IDF vectorizer has a parameter which allows for the extraction of words that occur frequently together, such as \"machine learning\" or \"data science\". These are referred to as bigrams and can also be useful in identifying key concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words to stopwords list\n",
    "custom_stopwords = ['bachelor', 'degree', 'ability', 'work', 'equal', 'opportunity', 'employer', 'objectives', 'ontario', 'canada', 'disability']\n",
    "\n",
    "# Initialize TFIDF Vectorizer\n",
    "tvec2 = TfidfVectorizer(analyzer = 'word', \n",
    "                       stop_words = ENGLISH_STOP_WORDS.union(custom_stopwords), \n",
    "                       lowercase= True, \n",
    "                       ngram_range=(2,3), \n",
    "                       min_df=4)\n",
    "\n",
    "# returns matrix of tf-idf features\n",
    "tvec2_token = tvec2.fit_transform(df_data['jobdescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           tfidf\n",
      "communication skills    1.891112\n",
      "data analysis           2.011601\n",
      "years experience        2.081029\n",
      "problem solving         2.100832\n",
      "business intelligence   2.366535\n",
      "computer science        2.429336\n",
      "internal external       2.438639\n",
      "team members            2.457507\n",
      "data analytics          2.486495\n",
      "experience working      2.536756\n",
      "project management      2.547119\n",
      "experience data         2.589679\n",
      "ad hoc                  2.589679\n",
      "decision making         2.600608\n",
      "business requirements   2.600608\n",
      "related field           2.611658\n",
      "minimum years           2.634131\n",
      "problem solving skills  2.668816\n",
      "analytical skills       2.668816\n",
      "solving skills          2.668816\n",
      "written communication   2.668816\n",
      "business analyst        2.680651\n",
      "job description         2.680651\n",
      "selection process       2.704748\n",
      "data management         2.729441\n"
     ]
    }
   ],
   "source": [
    "weights2 = dict(zip(tvec2.get_feature_names(), tvec2.idf_))\n",
    "tfidf2 = pd.DataFrame.from_dict(weights2, orient='index')\n",
    "tfidf2.columns = ['tfidf']\n",
    "\n",
    "# Lowest TFIDF Scores\n",
    "low_tfidf2 = tfidf2.sort_values(by=['tfidf'], ascending=True).head(25)\n",
    "print(low_tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             tfidf\n",
      "00 hourexperience         5.502029\n",
      "growing business          5.502029\n",
      "gym membership family     5.502029\n",
      "studios wattpad           5.502029\n",
      "studios wattpad books     5.502029\n",
      "guidance support          5.502029\n",
      "submitted updated         5.502029\n",
      "growth initiatives        5.502029\n",
      "growth employee wellness  5.502029\n",
      "growth employee           5.502029\n"
     ]
    }
   ],
   "source": [
    "# Highest TFIDF Scores\n",
    "high_tfidf2 = tfidf2.sort_values(by=['tfidf'], ascending=False).head(10)\n",
    "print(high_tfidf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text Pre-processing Pipeline Function</h3>\n",
    "\n",
    "Put it together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = ['bachelor', 'degree', 'work', 'equal', 'opportunity', 'employer', 'objectives', 'ontario', 'canada', 'disability', 'strong', 'including', 'ensure', 'understanding', 'related']\n",
    "\n",
    "# Initialize TFIDF Vectorizer\n",
    "tvec3 = TfidfVectorizer(analyzer = 'word',  \n",
    "                       stop_words = ENGLISH_STOP_WORDS.union(custom_stopwords), \n",
    "                       lowercase= True, \n",
    "                       ngram_range=(1,3),\n",
    "                       min_df=4)\n",
    "\n",
    "def tfidf3_pipeline(txt):\n",
    "    x = tvec3.fit_transform(txt) # Apply Vectorizer, Stopword Removal, & Lowercasing\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtext_TFIDF3 = tfidf3_pipeline(df_data['jobdescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original raw data df shape: (450, 2)\n",
      "Preprocessed data shape: (450, 17190)\n"
     ]
    }
   ],
   "source": [
    "original = df_data.shape\n",
    "preprocessed = jobtext_TFIDF3.shape\n",
    "\n",
    "print(\"Original raw data df shape: \" + str(original))\n",
    "print(\"Preprocessed data shape: \" + str(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  tfidf\n",
      "data           1.008909\n",
      "experience     1.061734\n",
      "business       1.105114\n",
      "skills         1.115015\n",
      "team           1.137658\n",
      "management     1.242170\n",
      "analysis       1.256395\n",
      "years          1.259265\n",
      "analyst        1.327642\n",
      "support        1.330724\n",
      "process        1.333815\n",
      "knowledge      1.355725\n",
      "ability        1.374895\n",
      "working        1.391156\n",
      "requirements   1.431295\n",
      "communication  1.434714\n",
      "tools          1.458978\n",
      "development    1.469560\n",
      "solutions      1.491066\n",
      "job            1.509349\n",
      "analytical     1.516756\n",
      "role           1.516756\n",
      "environment    1.524219\n",
      "provide        1.524219\n",
      "information    1.539313\n"
     ]
    }
   ],
   "source": [
    "weights4 = dict(zip(tvec3.get_feature_names(), tvec3.idf_))\n",
    "tfidf3 = pd.DataFrame.from_dict(weights4, orient='index')\n",
    "tfidf3.columns = ['tfidf']\n",
    "\n",
    "# Lowest TFIDF Scores\n",
    "low_tfidf3 = tfidf3.sort_values(by=['tfidf'], ascending=True).head(25)\n",
    "print(low_tfidf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-jobmarket",
   "language": "python",
   "name": "nlp-jobmarket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
