{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Market Opportunities in Toronto\n",
    "## For Data Analyst and Data Scientist Positions\n",
    "\n",
    "By: Jennifer Ma\n",
    "Github: jma199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is a useful asset to a company and has been demonstrated to be key in driving companies' profits. With the ubiquity of the internet and social media, data is now easily available to companies, large and small. As a result, there has also been an increasing demand in positions for people to provide data-driven insights.\n",
    "\n",
    "As part of my own job search, I wanted to know what jobs are available in Toronto area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "From the job-board website Indeed, information from job postings for data analyst and data scientist jobs will be scraped using Scrapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import time\n",
    "\n",
    "# you may need to install protego in order for the spider to work\n",
    "#!pip install protego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spider class\n",
    "class indeedanalystspider(scrapy.Spider):\n",
    "    name = \"indeedanalystspider\"\n",
    "    allowed_domains = ['indeed.com']\n",
    "    \n",
    "    # method to request urls\n",
    "    def start_requests(self):\n",
    "        url = \"https://ca.indeed.com/jobs?q=Data%20Analyst&l=Toronto%2C%20ON\"\n",
    "        yield scrapy.Request(url = url,\n",
    "                            callback = self.parse_front)\n",
    "    \n",
    "    # parse list of returned jobs and save links from page 1\n",
    "    page_count = 1\n",
    "    def parse_front(self, response):\n",
    "        # identify each job card #mosaic-provider-jobcards\n",
    "        job_cards = response.css('div[contains(@class,\"mosaic mosaic-provider-jobcards\")]')\n",
    "        # find each job link\n",
    "        job_links = job_cards.xpath('./a/@href')\n",
    "        # extract list of links to follow\n",
    "        links_to_follow = job_links.getall()\n",
    "        \n",
    "        # extract url for next page from \">\" arrow\n",
    "        #link_next_page = response.css('.pagination-list > li:nth-child(6) > a:nth-child(1)').get()\n",
    "        #if next_page is not None:\n",
    "         #   time.sleep(0.2)\n",
    "          #  page_count += 1\n",
    "           # yield scrapy.follow(a, callback = self.parse_front)\n",
    "        \n",
    "        print(\"The page count is: \", page_count)\n",
    "        print(f\"Job URL scraping completed: {len(links_to_follow)}\", end = '\\n')\n",
    "        return links_to_follow\n",
    "        \n",
    "        #follow link to next page\n",
    "        #for url in links_to_follow:\n",
    "         #   time.sleep(0.1)\n",
    "          #  yield response.follow(url = url,\n",
    "           #                      callback = self.parse_jobpage)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "# Initialize job list\n",
    "job_list = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# parse individual jobs\n",
    "    def parse_jobpage(self, response):\n",
    "        # Create SelectorList of job titles\n",
    "        job_titles = response.css('h1[contains(@class, \"jobsearch-JobInfoHeader-title\")]::text')\n",
    "        job_titles_ext = job_titles.get().strip()\n",
    "        \n",
    "        # Company .icl-u-lg-mr--sm\n",
    "        company = response.css('div[contains(@class, \"jobsearch-DesktopStickyContainer-company\")] > div::text')\n",
    "        company_ext = company.get().strip()\n",
    "        \n",
    "        # Location .jobsearch-JobInfoHeader-subtitle > div:nth-child(2)\n",
    "        location = response.css('div[contains(@class, \"jobsearch-JobInfoHeader-subtitle\")] > div:nth-of-type(2)')\n",
    "        location_ext = location.get().strip()\n",
    "        \n",
    "        # Salary\n",
    "        salary = response.css('span.icl-u-xs-mr--xs::text')\n",
    "        salary_ext = salary.get().strip()\n",
    "        \n",
    "        # Create SelectorList of job descriptions\n",
    "        job_desc = response.css('#jobDescriptionText::text')\n",
    "        job_desc_ext = job_desc.get().strip()\n",
    "        \n",
    "        # Fill job_list\n",
    "        job_info = {'jobtitle': job_title,\n",
    "               'company': company,\n",
    "               'location': job_location,\n",
    "               'salary': job_salary,\n",
    "               'jobdescription': job_description\n",
    "               }\n",
    "        job_list.append(job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 15:18:13 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
      "2021-11-23 15:18:13 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.5 (default, Sep  4 2020, 02:22:02) - [Clang 10.0.0 ], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform macOS-10.16-x86_64-i386-64bit\n",
      "2021-11-23 15:18:13 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2021-11-23 15:18:13 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2021-11-23 15:18:13 [scrapy.extensions.telnet] INFO: Telnet Password: 5ce66ca2fe059ed7\n",
      "2021-11-23 15:18:13 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-11-23 15:18:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-11-23 15:18:13 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-11-23 15:18:13 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-11-23 15:18:13 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-11-23 15:18:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-11-23 15:18:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-11-23 15:18:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://ca.indeed.com/jobs?q=Data%20Analyst&l=Toronto,%20ON> from <GET https://ca.indeed.com/jobs?q=Data%20Analyst&l=Toronto%2C%20ON>\n",
      "2021-11-23 15:18:14 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://ca.indeed.com/jobs?q=Data%20Analyst&l=Toronto,%20ON> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n",
      "2021-11-23 15:18:14 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-11-23 15:18:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 256,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 577,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'dupefilter/filtered': 1,\n",
      " 'elapsed_time_seconds': 0.236864,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 11, 23, 20, 18, 14, 200103),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 322420736,\n",
      " 'memusage/startup': 322420736,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2021, 11, 23, 20, 18, 13, 963239)}\n",
      "2021-11-23 15:18:14 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Run scrapy spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(indeedanalystspider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'links_to_follow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9134295cf96c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# convert list of job URLs to df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0murl_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"links to follow\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinks_to_follow\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0murl_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'links_to_follow' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# convert list of job URLs to df\n",
    "url_df = pd.DataFrame({\"links to follow\": links_to_follow})\n",
    "url_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-jobmarket",
   "language": "python",
   "name": "nlp-jobmarket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
