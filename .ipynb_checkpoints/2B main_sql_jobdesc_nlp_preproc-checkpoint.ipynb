{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2 : Pre-Processing of Job Description Text</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Import Packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# If you haven't already done so, execute:\n",
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Load data from sqlite database</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data']\n",
      "['jobtitle', 'company', 'location', 'salary', 'jobdescription', 'label']\n",
      "Table('data', MetaData(bind=None), Column('jobtitle', VARCHAR(length=100), table=<data>), Column('company', VARCHAR(length=100), table=<data>), Column('location', VARCHAR(length=25), table=<data>), Column('salary', INTEGER(), table=<data>), Column('jobdescription', TEXT(), table=<data>), Column('label', INTEGER(), table=<data>), schema=None)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "\n",
    "# Query SQL db to get analyst job descriptions first\n",
    "\n",
    "# Create connection to db\n",
    "engine = create_engine(\"sqlite:///joblist.sqlite\")\n",
    "print(engine.table_names())\n",
    "\n",
    "# Load in data table\n",
    "metadata = MetaData()\n",
    "data = Table('data', metadata, autoload=True, autoload_with=engine)\n",
    "\n",
    "print(data.columns.keys())\n",
    "print(repr(metadata.tables['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jobdescription', 'label']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "# Build query\n",
    "stmt = select([data.columns.jobdescription, data.columns.label])\n",
    "stmt = stmt.where(data.columns.label == '0') # 0 = analysts\n",
    "\n",
    "# Create connection to engine\n",
    "connection = engine.connect()\n",
    "\n",
    "# Execute query\n",
    "results = connection.execute(stmt).fetchall()\n",
    "print(results[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in db\n",
    "\n",
    "from sqlalchemy import func\n",
    "\n",
    "stmt_count = select([func.count(data.columns.jobdescription)])\n",
    "results_count = connection.execute(stmt_count).scalar()\n",
    "print(results_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      jobdescription  label\n",
      "0  Position Title:Pricing Analyst Position Type: ...      0\n",
      "1  Title: Senior Data Analyst - Telephony Manager...      0\n",
      "2  We are looking for a talented Fuel Cell Data E...      0\n",
      "3  CAREER OPPORTUNITY SENIOR METER DATA ANALYST L...      0\n",
      "4  The Data Engineer reports directly to the Dire...      0\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe from SQLAlchemy ResultSet\n",
    "df_data = pd.DataFrame(results)\n",
    "\n",
    "# Give columns proper heading\n",
    "df_data.columns = results[0].keys()\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 450 entries, 0 to 449\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   jobdescription  450 non-null    object\n",
      " 1   label           450 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 7.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 450 entries, 0 to 449\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   jobdescription  450 non-null    string\n",
      " 1   label           450 non-null    int64 \n",
      "dtypes: int64(1), string(1)\n",
      "memory usage: 7.2 KB\n"
     ]
    }
   ],
   "source": [
    "df_data['jobdescription'] = df_data['jobdescription'].astype('string')\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Analysis of Job Description</h2>\n",
    "\n",
    "<h3>Pre-processing Pipeline</h3>\n",
    "\n",
    "For each job description, we will tokenize, stem, remove stop words, and lowercase each word.\n",
    "\n",
    "The following is an example using 1 job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select an example job description from df_data\n",
    "\n",
    "text = df_data['jobdescription'][7]\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n"
     ]
    }
   ],
   "source": [
    "# Tokenize example text\n",
    "# Maybe add len(token) to dataframe and plot\n",
    "\n",
    "text_token = word_tokenize(str(text))\n",
    "print(len(text_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'Preferred', ')', 'Work', 'remotely', ':', 'Temporarily', 'due', 'to', 'COVID-19COVID-19', 'precaution', '(', 's', ')', ':', 'Remote', 'interview', 'processPersonal', 'protective', 'equipment', 'provided', 'or', 'requiredPlastic', 'shield', 'at', 'work', 'stationsSocial', 'distancing', 'guidelines', 'in', 'placeVirtual', 'meetingsSanitizing', ',', 'disinfecting', ',', 'or', 'cleaning', 'procedures', 'in', 'place']\n"
     ]
    }
   ],
   "source": [
    "print(text_token[-40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'prefer', ')', 'work', 'remot', ':', 'temporarili', 'due', 'to', 'covid-19covid-19', 'precaut', '(', 's', ')', ':', 'remot', 'interview', 'processperson', 'protect', 'equip', 'provid', 'or', 'requiredplast', 'shield', 'at', 'work', 'stationssoci', 'distanc', 'guidelin', 'in', 'placevirtu', 'meetingssanit', ',', 'disinfect', ',', 'or', 'clean', 'procedur', 'in', 'place']\n"
     ]
    }
   ],
   "source": [
    "# stem tokens\n",
    "# stemmer takes a list an input\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text_stemmed = [stemmer.stem(w) for w in text_token]\n",
    "print(text_stemmed[-40:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Pre-processing Pipeline</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the job postings currently available will be considered the \"test\" set and will not be split. Any predictions will be made on new postings scraped at a later date.\n",
    "\n",
    "Here, we will try 2 different vectorizers: CountVectorizer and TF-IDF.\n",
    "\n",
    "The pre-processing pipeline will include the following steps:\n",
    "\n",
    "- Tokenization\n",
    "- Stopword removal\n",
    "- Lower casing\n",
    "- Stemming\n",
    "- Apply transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Count Vectorizer</h2>\n",
    "\n",
    "The CountVectorizer function returns an encoded vector with an integer count for each word.\n",
    "\n",
    "We will use CountVectorizer to identify skills that are commonly found in job descriptions. These skills would therefore have a higher count across the corpus compared to other (i) lesser used or company-specific skills and (ii) non-important words.\n",
    "\n",
    "Here, we will tokenize only words, use the built-in stop words list, and keep only tokens that appear in at least 2 dfs (document frequencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '00 hourexperience', '00 hourschedule', '00 yearadditional', '00 yearbenefits', '000', '000 00', '000 employees', '000 members', '000 products']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='word', \n",
    "                     #token_pattern = r\"[a-zA-Z]+\", \n",
    "                     stop_words = 'english', \n",
    "                     lowercase = True, \n",
    "                     min_df=4, \n",
    "                     ngram_range=(1,2))\n",
    "count_vector = cv.fit_transform(df_data['jobdescription'])\n",
    "\n",
    "print(cv.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  00 hourexperience  00 hourschedule  00 yearadditional  00 yearbenefits  \\\n",
      "0   0                  0                0                  0                0   \n",
      "1   0                  0                0                  0                0   \n",
      "2   0                  0                0                  0                0   \n",
      "3   0                  0                0                  0                0   \n",
      "4   0                  0                0                  0                0   \n",
      "\n",
      "   000  000 00  000 employees  000 members  000 products  ...  étapes  \\\n",
      "0    1       0              1            0             0  ...       0   \n",
      "1    0       0              0            0             0  ...       0   \n",
      "2    0       0              0            0             0  ...       0   \n",
      "3    0       0              0            0             0  ...       0   \n",
      "4    0       0              0            0             0  ...       0   \n",
      "\n",
      "   étapes car  éventail  éventail disciplines  êtes  êtes valorisé  être  \\\n",
      "0           0         0                     0     0              0     0   \n",
      "1           0         0                     0     0              0     0   \n",
      "2           0         0                     0     0              0     0   \n",
      "3           0         0                     0     0              0     0   \n",
      "4           0         0                     0     0              0     0   \n",
      "\n",
      "   être commune  être nous  œuvre  \n",
      "0             0          0      0  \n",
      "1             0          0      0  \n",
      "2             0          0      0  \n",
      "3             0          0      0  \n",
      "4             0          0      0  \n",
      "\n",
      "[5 rows x 12195 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe out of CV sparse array\n",
    "counts = pd.DataFrame(count_vector.toarray(), columns = cv.get_feature_names())\n",
    "print(counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968087831989431\n"
     ]
    }
   ],
   "source": [
    "# calculate sparsity\n",
    "sparsity = 1.0 - np.count_nonzero(counts) / counts.size\n",
    "print(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cv.stop_words_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TFIDF Vectorizer</h2>\n",
    "\n",
    "TFIDF converts the job description text into a matrix of TF-IDF features. The TF-IDF Vectorizer function from the sklearn.feature_extraction module performs multiple steps including tokenization, stopword removal, and lower casing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1. Initialize Stopword Removal & Lowercasing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words to stopwords list\n",
    "# https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list\n",
    "custom_stopwords = ['bachelor', 'degree', 'work', 'equal', 'opportunity', 'employer', 'objectives', 'ontario', 'canada', 'disability', 'strong', 'including', 'ensure', 'understanding', 'related']\n",
    "\n",
    "# Initialize TFIDF Vectorizer\n",
    "tvec = TfidfVectorizer(analyzer = 'word', \n",
    "                       #token_pattern = r\"[a-zA-Z]+\", \n",
    "                       stop_words = ENGLISH_STOP_WORDS.union(custom_stopwords), \n",
    "                       lowercase= True, \n",
    "                       min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Apply TF-IDF</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns matrix of tf-idf features\n",
    "tvec_token = tvec.fit_transform(df_data['jobdescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 6452)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 10256)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tvec_token.shape without token_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  tfidf\n",
      "data           1.008909\n",
      "experience     1.061734\n",
      "business       1.105114\n",
      "skills         1.115015\n",
      "team           1.137658\n",
      "management     1.242170\n",
      "analysis       1.256395\n",
      "years          1.259265\n",
      "analyst        1.327642\n",
      "support        1.330724\n",
      "process        1.333815\n",
      "knowledge      1.355725\n",
      "ability        1.374895\n",
      "working        1.391156\n",
      "requirements   1.431295\n",
      "communication  1.434714\n",
      "tools          1.458978\n",
      "development    1.469560\n",
      "solutions      1.491066\n",
      "job            1.509349\n",
      "analytical     1.516756\n",
      "role           1.516756\n",
      "environment    1.524219\n",
      "provide        1.524219\n",
      "information    1.539313\n",
      "reporting      1.550786\n",
      "time           1.578078\n",
      "new            1.598039\n",
      "projects       1.626670\n",
      "systems        1.635004\n",
      "required       1.635004\n",
      "processes      1.643407\n",
      "stakeholders   1.656146\n",
      "develop        1.660429\n",
      "key            1.673388\n",
      "sql            1.673388\n",
      "technical      1.690932\n",
      "reports        1.695367\n",
      "project        1.704296\n",
      "needs          1.704296\n",
      "written        1.708790\n",
      "excel          1.726972\n",
      "technology     1.736189\n",
      "internal       1.736189\n",
      "position       1.778749\n",
      "identify       1.783591\n",
      "teams          1.803200\n",
      "services       1.803200\n",
      "quality        1.808162\n",
      "design         1.808162\n"
     ]
    }
   ],
   "source": [
    "# Observe TFIDF Weights\n",
    "\n",
    "weights = dict(zip(tvec.get_feature_names(), tvec.idf_))\n",
    "tfidf = pd.DataFrame.from_dict(weights, orient='index')\n",
    "tfidf.columns = ['tfidf']\n",
    "\n",
    "# Lowest TFIDF Scores\n",
    "low_tfidf = tfidf.sort_values(by=['tfidf'], ascending=True).head(50)\n",
    "print(low_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tfidf\n",
      "installs        6.012855\n",
      "kid             6.012855\n",
      "pcmd            6.012855\n",
      "organizes       6.012855\n",
      "continents      6.012855\n",
      "mixpanel        6.012855\n",
      "firsts          6.012855\n",
      "contender       6.012855\n",
      "containerslead  6.012855\n",
      "requi           6.012855\n"
     ]
    }
   ],
   "source": [
    "# Highest TFIDF Scores\n",
    "high_tfidf = tfidf.sort_values(by=['tfidf'], ascending=False).head(10)\n",
    "print(high_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bi-grams and Tri-grams</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words to stopwords list\n",
    "# https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list\n",
    "custom_stopwords = ['bachelor', 'degree', 'ability', 'work', 'equal', 'opportunity', 'employer', 'objectives', 'ontario', 'canada', 'disability']\n",
    "\n",
    "# Initialize TFIDF Vectorizer\n",
    "tvec2 = TfidfVectorizer(analyzer = 'word', \n",
    "                       #token_pattern = r\"[a-zA-Z]+\", \n",
    "                       stop_words = ENGLISH_STOP_WORDS.union(custom_stopwords), \n",
    "                       lowercase= True, \n",
    "                       ngram_range=(2,3), \n",
    "                       min_df=4)\n",
    "\n",
    "# returns matrix of tf-idf features\n",
    "tvec2_token = tvec2.fit_transform(df_data['jobdescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           tfidf\n",
      "communication skills    1.891112\n",
      "data analysis           2.011601\n",
      "years experience        2.081029\n",
      "problem solving         2.100832\n",
      "business intelligence   2.366535\n",
      "computer science        2.429336\n",
      "internal external       2.438639\n",
      "team members            2.457507\n",
      "data analytics          2.486495\n",
      "experience working      2.536756\n",
      "project management      2.547119\n",
      "experience data         2.589679\n",
      "ad hoc                  2.589679\n",
      "decision making         2.600608\n",
      "business requirements   2.600608\n",
      "related field           2.611658\n",
      "minimum years           2.634131\n",
      "problem solving skills  2.668816\n",
      "analytical skills       2.668816\n",
      "solving skills          2.668816\n",
      "written communication   2.668816\n",
      "business analyst        2.680651\n",
      "job description         2.680651\n",
      "selection process       2.704748\n",
      "data management         2.729441\n"
     ]
    }
   ],
   "source": [
    "weights2 = dict(zip(tvec2.get_feature_names(), tvec2.idf_))\n",
    "tfidf2 = pd.DataFrame.from_dict(weights2, orient='index')\n",
    "tfidf2.columns = ['tfidf']\n",
    "\n",
    "# Lowest TFIDF Scores\n",
    "low_tfidf2 = tfidf2.sort_values(by=['tfidf'], ascending=True).head(25)\n",
    "print(low_tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             tfidf\n",
      "00 hourexperience         5.502029\n",
      "growing business          5.502029\n",
      "gym membership family     5.502029\n",
      "studios wattpad           5.502029\n",
      "studios wattpad books     5.502029\n",
      "guidance support          5.502029\n",
      "submitted updated         5.502029\n",
      "growth initiatives        5.502029\n",
      "growth employee wellness  5.502029\n",
      "growth employee           5.502029\n"
     ]
    }
   ],
   "source": [
    "# Highest TFIDF Scores\n",
    "high_tfidf2 = tfidf2.sort_values(by=['tfidf'], ascending=False).head(10)\n",
    "print(high_tfidf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text Pre-processing Pipeline Function</h3>\n",
    "\n",
    "See DSDJ Feature Engineering pt 2 for a train & test version of the Tfidf vectorizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that cleans and performs a TFIDF transformation to text data\n",
    "tfidf = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tfidf_pipeline(txt):\n",
    "    txt = txt.apply(porter.stem) # Apply Stemming\n",
    "    x = tfidf.fit_transform(txt) # Apply Vectorizer, Stopword Removal, & Lowercasing\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtext_TFIDF = tfidf_pipeline(jobText['Job Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = jobText.shape\n",
    "preprocessed = jobtext_TFIDF.shape\n",
    "\n",
    "print(\"Original raw data df shape: \" + str(original))\n",
    "print(\"Preprocessed data shape: \" + str(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobText_TFIDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency : how many words per post.\n",
    "# Text cleaning : lower casing, extra white-space removal, lemmatization\n",
    "\n",
    "# Determine most common words that occur in the job descriptions. \n",
    "# Predetermine a list of expected lookup terms for dictionary of skills\n",
    "\n",
    "# BOW - Create a list of dictionaries containing word counts for each job posting\n",
    "\n",
    "# Table with skill, count, percentage\n",
    "\n",
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words2Vec - similar words are closer together in a sentence\n",
    "\n",
    "# Topic modelling - where skills is considered a topic\n",
    "\n",
    "# NER with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-jobmarket",
   "language": "python",
   "name": "nlp-jobmarket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
