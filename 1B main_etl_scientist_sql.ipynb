{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Job Market Trends</h1>\n",
    "<h2>Extract, Transform, and Load Data</h2>\n",
    "\n",
    "Add Data Scientist job postings to database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1: Access data files within a Directory</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2 : Opening and extracting information from files</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 3 : Put it all together</h2>\n",
    "\n",
    "Put all the steps together so that we can easily extract job information from each text file and keep a record of which files we have opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, Text, insert, select, delete\n",
    "\n",
    "def get_raw_data(directory):\n",
    "    '''Open file containing html of job description and prepare soup object.'''\n",
    "    fileList = []\n",
    "    soupList = []\n",
    "    # Iterate through each file in directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            # add each filename to list\n",
    "            fileList.append(file)\n",
    "            #print(fileList)\n",
    "            # open and load html\n",
    "            with codecs.open(directory + \"/\"+ file, 'r', \"utf-8\") as f:\n",
    "                job_html = f.read()\n",
    "                job_soup = BeautifulSoup(job_html, \"html.parser\")\n",
    "                soupList.append(job_soup)\n",
    "    print(\"soup_list is done.\")\n",
    "    return soupList\n",
    "\n",
    "# From the loaded text, extract job information using beautiful soup\n",
    "def get_job_record(job_soup):\n",
    "    '''Create a record of information for one job.'''\n",
    "    # Title\n",
    "    try:\n",
    "        job_title = job_soup.find('h1').text.strip()\n",
    "    except:\n",
    "        job_title = \"NaN\"\n",
    "    \n",
    "    # Company\n",
    "    try:\n",
    "        company = job_soup.find(\"div\", class_=\"jobsearch-InlineCompanyRating\").next_element.next_element.text.strip()\n",
    "    except:    \n",
    "        try:\n",
    "            company = job_soup.find(\"div\", class_=\"jobsearch-InlineCompanyRating\").text.strip()\n",
    "        except:\n",
    "            company = \"NaN\"\n",
    "\n",
    "    # Location\n",
    "    try:\n",
    "        job_location = job_soup.find(\"div\", class_ = \"jobsearch-InlineCompanyRating\").next_sibling.text.strip()\n",
    "    except:\n",
    "        try:\n",
    "            job_location = job_soup.find(\"div\", class_ = \"jobsearch-InlineCompanyRating\").next_sibling.next_sibling.text.strip()\n",
    "        except:\n",
    "            job_location = 'NaN'\n",
    "\n",
    "    # Job Description\n",
    "    try:\n",
    "        job_description = job_soup.find(\"div\", class_=\"jobsearch-jobDescriptionText\").text.strip()\n",
    "    except:\n",
    "        job_description = \"NaN\"\n",
    "\n",
    "    # Not all postings have a salary available\n",
    "    try:\n",
    "        job_salary = job_soup.find(\"span\", class_=\"icl-u-xs-mr--xs\").text.strip()\n",
    "    except AttributeError:\n",
    "        job_salary = \"NaN\"\n",
    "    \n",
    "    job_record = {'jobtitle': job_title,\n",
    "                  'company': company,\n",
    "                  'location': job_location,\n",
    "                  'salary': job_salary,\n",
    "                  'jobdescription': job_description,\n",
    "                  'label': 1\n",
    "                 }\n",
    "    return job_record\n",
    "\n",
    "def main_etl(directory):\n",
    "    '''This function loads text data, extracts pertinent job information, and saves data in a sql database.'''\n",
    "    soupList = get_raw_data(directory)\n",
    "        \n",
    "    # add each job record to a list\n",
    "    # this will create a list of dictionaries, making it easy to insert into a sql table\n",
    "    job_records = []\n",
    "    for job_soup in soupList:\n",
    "        job_record = get_job_record(job_soup)\n",
    "        job_records.append(job_record)\n",
    "        print(\"Added to job_records list. Length of job_records is: \", len(job_records))\n",
    "\n",
    "    # add job records to sqlite db\n",
    "    # Create engine: engine\n",
    "    engine = create_engine('sqlite:///joblist.sqlite')\n",
    "    metadata = MetaData()\n",
    "\n",
    "    # Define a new table\n",
    "    data = Table('data', metadata,\n",
    "                 Column('jobtitle', String(100)),\n",
    "                 Column('company', String(100)),\n",
    "                 Column('location', String(25)),\n",
    "                 Column('salary', Integer()),\n",
    "                 Column('jobdescription', Text()),\n",
    "                 Column('label', Integer())\n",
    "                )\n",
    "\n",
    "    # Create table\n",
    "    metadata.create_all(engine)\n",
    "\n",
    "    # Print table details\n",
    "    print(engine.table_names())\n",
    "\n",
    "    # Build an insert statement to insert a record into the data table: insert_stmt\n",
    "    insert_stmt = insert(data)\n",
    "\n",
    "    # Execute the insert statement via the connection: results\n",
    "    connection = engine.connect()\n",
    "    results = connection.execute(insert_stmt, job_records)\n",
    "\n",
    "    # Print result rowcount\n",
    "    print(\"The number of rows added is: \", results.rowcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jennifer/nlp-jobmarket\n",
      "\u001b[1m\u001b[36mData Analyst\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mData Scientist\u001b[m\u001b[m\r\n",
      "README.md\r\n",
      "default-a326eba2-8d86-4f82-add7-16670adff2d4.ipynb\r\n",
      "joblist.sqlite\r\n",
      "\u001b[31mmain_etl_analyst.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mmain_etl_analyst_sql.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mmain_etl_scientist_sql.ipynb\u001b[m\u001b[m\r\n",
      "main_etl_scientist_sql.py\r\n",
      "\u001b[31mmain_jobdesc_eda.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mmain_jobdesc_preproc copy.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mmain_jobdesc_preproc.ipynb\u001b[m\u001b[m\r\n",
      "results.csv\r\n",
      "\u001b[1m\u001b[36mtest_folder\u001b[m\u001b[m\r\n",
      "\u001b[30m\u001b[43mtest_folder2\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soup_list is done.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'job_description' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7429c2cbc329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataScientist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_etl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Scientist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-744071afe0d6>\u001b[0m in \u001b[0;36mmain_etl\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mjob_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mjob_soup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoupList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mjob_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_job_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_soup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mjob_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Added to job_records list. Length of job_records is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-744071afe0d6>\u001b[0m in \u001b[0;36mget_job_record\u001b[0;34m(job_soup)\u001b[0m\n\u001b[1;32m     67\u001b[0m                   \u001b[0;34m'location'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjob_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                   \u001b[0;34m'salary'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjob_salary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                   \u001b[0;34m'jobdescription'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjob_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                   \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                  }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'job_description' is not defined"
     ]
    }
   ],
   "source": [
    "dataScientist = main_etl(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
