{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Market Opportunities in Toronto\n",
    "## For Data Analyst and Data Scientist Positions\n",
    "\n",
    "__By: Jennifer Ma\n",
    "\n",
    "__Github: jma199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is a useful asset to a company and has been demonstrated to be key in driving companies' profits. With the ubiquity of the internet and social media, data is now easily available to companies, large and small. As a result, there has also been an increasing demand in positions for people to provide data-driven insights.\n",
    "\n",
    "As part of my own job search, I wanted to know what the differences between data analyst and data scientist positions jobs and what kinds of companies are hiring in Toronto area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "Information about available job openings can be found online at job-posting sites such as Indeed.\n",
    "HTML for each job posting was saved as a text file. \n",
    "To parse html data in text files, BeautifulSoup was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:69: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:168: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:69: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:168: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-2-691eca87c5d4>:69: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if position_type is \"analyst\":\n",
      "<ipython-input-2-691eca87c5d4>:168: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if position_type is \"analyst\":\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, Text, insert, select, delete\n",
    "\n",
    "# The load_data function is used by the main_etl function to load data from a directory\n",
    "def load_data(directory):\n",
    "    '''Given a directory, open each text file containing html of job description and prepare soupList object.'''\n",
    "    fileList = []\n",
    "    soupList = []\n",
    "    # Iterate through each file in directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            # add each filename to list\n",
    "            fileList.append(file)\n",
    "            #print(fileList)\n",
    "            # open and load html\n",
    "            with codecs.open(directory + \"/\"+ file, 'r', \"utf-8\") as f:\n",
    "                job_html = f.read()\n",
    "                job_soup = BeautifulSoup(job_html, \"html.parser\")\n",
    "                soupList.append(job_soup)\n",
    "    print(\"Data is loaded.\")\n",
    "    return soupList\n",
    "\n",
    "# The get_job_record is used by the main_etl function to extract job information\n",
    "# From the loaded text, extract job information using beautiful soup\n",
    "def get_job_record(job_soup, position_type):\n",
    "    '''Create a dictionary containing a record of information for one job. \n",
    "    Each record contains title, company, location, job description, and salary (if available).'''\n",
    "\n",
    "    position_types = ['analyst', 'scientist']\n",
    "    if position_type not in position_types:\n",
    "        raise ValueError(\"Invalid position_type. Expected one of: {}.\".format(position_types))\n",
    "    \n",
    "    # Title\n",
    "    try:\n",
    "        job_title = job_soup.find('h1').text.strip()\n",
    "    except:\n",
    "        job_title = \"NaN\"\n",
    "    \n",
    "    # Company\n",
    "    try:\n",
    "        company = job_soup.find(\"div\", class_=\"jobsearch-InlineCompanyRating\").next_element.next_element.text.strip()\n",
    "    except:    \n",
    "        try:\n",
    "            company = job_soup.find(\"div\", class_=\"jobsearch-InlineCompanyRating\").text.strip()\n",
    "        except:\n",
    "            company = \"NaN\"\n",
    "\n",
    "    # Location\n",
    "    try:\n",
    "        job_location = job_soup.find(\"div\", class_ = \"jobsearch-InlineCompanyRating\").next_sibling.text.strip()\n",
    "    except:\n",
    "        try:\n",
    "            job_location = job_soup.find(\"div\", class_ = \"jobsearch-InlineCompanyRating\").next_sibling.next_sibling.text.strip()\n",
    "        except:\n",
    "            job_location = 'NaN'\n",
    "\n",
    "    # Job Description\n",
    "    try:\n",
    "        job_description = job_soup.find(\"div\", class_=\"jobsearch-jobDescriptionText\").text.strip().replace('\\n', ' ')\n",
    "    except:\n",
    "        job_description = \"NaN\"\n",
    "\n",
    "    # Not all postings have a salary available\n",
    "    try:\n",
    "        job_salary = job_soup.find(\"span\", class_=\"icl-u-xs-mr--xs\").text.strip()\n",
    "    except:\n",
    "        job_salary = \"NaN\"\n",
    "    \n",
    "    if position_type is \"analyst\":\n",
    "        label = 0\n",
    "    else:   #datascientist\n",
    "        label = 1\n",
    "        \n",
    "    job_record = {'jobtitle': job_title,\n",
    "                    'company': company,\n",
    "                    'location': job_location,\n",
    "                    'salary': job_salary,\n",
    "                    'jobdescription': job_description,\n",
    "                    'label': label\n",
    "                 }\n",
    "\n",
    "    return job_record\n",
    "\n",
    "def main_etl(directory, position_type):\n",
    "    '''Takes two arguments: directory and position type (\"analyst\" or \"scientist\").\n",
    "    Load text data, extract pertinent job information, and save data in a sql database.'''\n",
    "\n",
    "    soupList = load_data(directory)\n",
    "    # add each job record to a list\n",
    "    # this will create a list of dictionaries, making it easy to insert into sqlite table\n",
    "    job_records = []\n",
    "    for job_soup in soupList:\n",
    "        job_record = get_job_record(job_soup, position_type)\n",
    "        job_records.append(job_record)\n",
    "        #print(\"Added to job_records list. Length of job_records is: \", len(job_records))\n",
    "\n",
    "    # add job records to sqlite db\n",
    "    # Create engine: engine\n",
    "    engine = create_engine('sqlite:///joblist.sqlite')\n",
    "    metadata = MetaData()\n",
    "\n",
    "    # Define a new table\n",
    "    data = Table('data2', metadata,\n",
    "                 Column('jobtitle', String(100)),\n",
    "                 Column('company', String(100)),\n",
    "                 Column('location', String(25)),\n",
    "                 Column('salary', Integer()),\n",
    "                 Column('jobdescription', Text()),\n",
    "                 Column('label', Integer())\n",
    "                )\n",
    "\n",
    "    # Create table\n",
    "    metadata.create_all(engine)\n",
    "    print(engine.table_names())\n",
    "\n",
    "    # Insert results into table\n",
    "    insert_stmt = insert(data)\n",
    "    connection = engine.connect()\n",
    "    results = connection.execute(insert_stmt, job_records)\n",
    "\n",
    "    # Print resulting rowcount\n",
    "    print(\"The number of rows added is: \", results.rowcount)\n",
    "\n",
    "# add more job records after indeed made change to its code\n",
    "# used by update_db function\n",
    "def get_job_record_update(job_soup, position_type):\n",
    "    '''Create a record of information for one job.'''\n",
    "\n",
    "    position_types = ['analyst', 'scientist']\n",
    "    if position_type not in position_types:\n",
    "        raise ValueError(\"Invalid position_type. Expected one of: {}.\".format(position_types))\n",
    "    \n",
    "    # Title\n",
    "    try:\n",
    "        job_title = job_soup.find(\"div\", id=\"vjs-jobtitle\").text.strip()\n",
    "    except:\n",
    "        try:\n",
    "            job_title = job_soup.find(\"h1\", id=\"vjs-jobtitle\").text.strip()\n",
    "        except:\n",
    "            job_title = \"NaN\"\n",
    "    \n",
    "    # Company\n",
    "    try:\n",
    "        company = job_soup.find(\"span\", id=\"vjs-cn\").text.strip()\n",
    "    except:    \n",
    "        company = \"NaN\"\n",
    "\n",
    "    # Location\n",
    "    try:\n",
    "        job_location = job_soup.find(\"span\", id=\"vjs-loc\").text.strip().replace(\"- \", \"\")\n",
    "    except:\n",
    "        job_location = \"NaN\"\n",
    "    \n",
    "    # Job Salary\n",
    "    try:\n",
    "        job_salary = job_soup.find(\"span\", attrs = {\"id\": None, \"class\": None, \"aria-hidden\": None}).text.strip()\n",
    "    except:\n",
    "        job_salary = \"NaN\"\n",
    "    \n",
    "    # Job Description\n",
    "    try:\n",
    "        job_description = job_soup.find(\"div\", id=\"vsj-desc\").text.strip().replace(\"\\n\", \" \")\n",
    "    except:\n",
    "        try:\n",
    "            job_description = job_soup.find(\"div\", id=\"vjs-content\").text.strip().replace(\"\\n\", \" \")\n",
    "        except:\n",
    "            job_summary = \"NaN\"\n",
    "    \n",
    "    if position_type is \"analyst\":\n",
    "        label = 0\n",
    "    else:   #datascientist\n",
    "        label = 1\n",
    "    \n",
    "    job_record = {'jobtitle': job_title,\n",
    "                  'company': company,\n",
    "                  'location': job_location,\n",
    "                  'salary': job_salary,\n",
    "                  'jobdescription': job_description,\n",
    "                  'label': label\n",
    "                 }\n",
    "    return job_record\n",
    "\n",
    "\n",
    "# update database with more entries\n",
    "def update_db(directory, position_type, db_name):\n",
    "    '''This function loads text data, extracts pertinent job information, and adds data to an existing sql database.'''\n",
    "    \n",
    "    soupList = load_data(directory)\n",
    "        \n",
    "    # add each job record to a list\n",
    "    # this will create a list of dictionaries, making it easy to insert into a sql table\n",
    "    job_records = []\n",
    "    for job_soup in soupList:\n",
    "        job_record = get_job_record_update(job_soup, position_type)\n",
    "        job_records.append(job_record)\n",
    "        #print(\"Added to job_records list. Length of job_records is: \", len(job_records))\n",
    "    \n",
    "    print(\"Finished extracting information for job_records list.\")\n",
    "    \n",
    "    # add job records to existing sqlite db\n",
    "    # Create engine: engine\n",
    "    engine = create_engine(db_name)\n",
    "    metadata = MetaData()\n",
    "    \n",
    "    # Reflect table\n",
    "    data = Table('data2', metadata, autoload=True, autoload_with=engine)\n",
    "    \n",
    "    # Build an insert statement to insert a record into the data table: insert_stmt\n",
    "    insert_stmt = insert(data)\n",
    "\n",
    "    # Execute the insert statement via the connection: results\n",
    "    connection = engine.connect()\n",
    "    results = connection.execute(insert_stmt, job_records)\n",
    "\n",
    "    # Print result rowcount\n",
    "    print(\"The number of rows added is: \", results.rowcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run main etl function for both data analyst and data scientist postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded.\n",
      "['data', 'data2']\n",
      "The number of rows added is:  450\n"
     ]
    }
   ],
   "source": [
    "# add data from folder \"Data Analyst\"\n",
    "dataAnalyst = main_etl(\"Data Analyst\", \"analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded.\n",
      "['data', 'data2']\n",
      "The number of rows added is:  175\n"
     ]
    }
   ],
   "source": [
    "# add data from folder \"Data Scientist\"\n",
    "dataScientist = main_etl(\"Data Scientist\", \"scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded.\n",
      "Finished extracting information for job_records list.\n",
      "The number of rows added is:  75\n"
     ]
    }
   ],
   "source": [
    "# update database with more data scientist postings\n",
    "dataScientist = update_db('Data Scientist update', 'scientist', db_name = 'sqlite:///joblist.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid position_type. Expected one of: ['analyst', 'scientist'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9666f3161ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataAnalyst_errorcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_etl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/Data Analyst\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'engineer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-691eca87c5d4>\u001b[0m in \u001b[0;36mmain_etl\u001b[0;34m(directory, position_type)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mjob_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mjob_soup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoupList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mjob_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_job_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_soup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mjob_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m#print(\"Added to job_records list. Length of job_records is: \", len(job_records))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-691eca87c5d4>\u001b[0m in \u001b[0;36mget_job_record\u001b[0;34m(job_soup, position_type)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mposition_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'analyst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scientist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mposition_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposition_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid position_type. Expected one of: {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid position_type. Expected one of: ['analyst', 'scientist']."
     ]
    }
   ],
   "source": [
    "dataAnalyst_errorcheck = main_etl(\"Data/Data Analyst\", 'engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a47ea2542f6684f2610983aa0f4abea0242db1ce7569de787ba8ff23ff0a1a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dataenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
