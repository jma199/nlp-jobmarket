{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# If you haven't already done so, execute:\n",
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2>Count Vectorizer with Stemming Function</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the source code, \"if analyzer is used, only the decoder argument is used, as the analyzer is intended to replace the preprocessor, tokenizer, and ngrams steps.\"\n",
    "\n",
    "Here, we will use a custom function for the analyzer parameter instead of using 'word'. This results in the token_pattern parameter to be ignored (as per the documentation). This is also true for stop_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build custom stemmer to use with CountVectorizer\n",
    "# Used stack overflow post as guide\n",
    "# https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn\n",
    "# 20 comments\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "#analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "class CountVectorizerStemmed(CountVectorizer):\n",
    "    def custom_analyzer(self):\n",
    "        analyzer = super(CountVectorizerStemmed, self).build_analyzer()\n",
    "        return lambda text: [stemmer.stem(w) for w in analyzer(text)]\n",
    "\n",
    "cv_stem = CountVectorizerStemmed(stop_words = 'english', \n",
    "                                 lowercase = True, \n",
    "                                 min_df=2,\n",
    "                                token_pattern = r\"[a-zA-Z]+\")\n",
    "vector_stem = cv_stem.fit_transform(df_data['jobdescription'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including token_pattern parameter here did result in the removal of numbers from features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_stem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_stem.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_stem = pd.DataFrame(vector_stem.toarray(), columns = cv_stem.get_feature_names())\n",
    "counts_stem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that this did not result in the stemming of any of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.search(r\"[a-zA-Z]+\")    \n",
    "\n",
    "# list of punctuation\n",
    "punc = string.punctuation    \n",
    "\n",
    "#try combining stopwords and punctuation together\n",
    "user_defined_stop_words = ['st','rd','hong','kong']     \n",
    "i = nltk.corpus.stopwords.words('english')\n",
    "j = list(string.punctuation) + user_defined_stop_words\n",
    "\n",
    "stopwords = set(i).union(j)\n",
    "\n",
    "# or try this\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31 comments\n",
    "# Try using a function\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "cv_stem2 = CountVectorizer(analyzer=stemmed_words, \n",
    "                           stop_words = 'english', \n",
    "                           lowercase = True, \n",
    "                           min_df=2,\n",
    "                           token_pattern = r\"[a-zA-Z]+\")\n",
    "count_vector2 = cv_stem2.fit_transform(df_data['jobdescription'])\n",
    "\n",
    "print(cv_stem2.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is noticeably slow due to the lambda function.\n",
    "Including token_pattern parameter here did not result in only word features (numbers are still there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_stem2 = pd.DataFrame(count_vector2.toarray(), columns = cv_stem2.get_feature_names())\n",
    "counts_stem2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dataenv': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd013f2b1e521b8f6e58ac5f308ce6e9ec48daaa5d3d12011623b022ddafac4d33a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
